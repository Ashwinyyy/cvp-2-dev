import boto3
import json
import aiohttp
import asyncio
from botocore.exceptions import ClientError
from datetime import datetime, timezone, timedelta
import os
import shutil

# Initialize the S3 client and SNS client
s3_client = boto3.client('s3')
sns_client = boto3.client('sns')

# Function to fetch report details asynchronously
async def fetch_report_details(session, report_id, api_maintenance_sns_arn, error_notification_sent):
    url = f"https://health-products.canada.ca/api/canada-vigilance/report/{report_id}?lang=en&type=json"
    try:
        async with session.get(url) as response:
            if response.status != 200:
                raise Exception(f"API is under maintenance or returned status code {response.status}")
            try:
                return await response.json()  # Try to parse the response as JSON
            except aiohttp.ContentTypeError as e:
                error_message = (
                    "At present, we are unable to fetch the data from the API.\n\n"
                    f"API is under maintenance: {str(e)},"
                )
                if not error_notification_sent[0]:
                    send_sns_notification(api_maintenance_sns_arn, error_message, "API Maintenance Alert")
                    error_notification_sent[0] = True
                raise Exception(f"API is under maintenance: {str(e)}")
    except Exception as e:
        error_message = (
            "Greetings. \n\n"
            "At present, we are unable to fetch Adverse reaction report from the API.\n\n"
            "Please Find the Error details Below. \n\n"
            f"Error Details: {str(e)}"
        )
        if not error_notification_sent[0]:
            send_sns_notification(api_maintenance_sns_arn, error_message, "API Maintenance Alert")
            error_notification_sent[0] = True
        raise

# Function to download file from S3
def download_file_from_s3(bucket_name, s3_key, local_file_path):
    try:
        s3_client.download_file(bucket_name, s3_key, local_file_path)
    except ClientError as e:
        print(f"Error downloading file from S3: {e}")
        raise

# Function to upload file to S3
def upload_file_to_s3(bucket_name, s3_key, local_file_path):
    try:
        s3_client.upload_file(local_file_path, bucket_name, s3_key)
    except ClientError as e:
        print(f"Error uploading file to S3: {e}")
        raise

# Function to send an SNS notification
def send_sns_notification(sns_topic_arn, message, subject="Notification"):
    try:
        response = sns_client.publish(
            TopicArn=sns_topic_arn,
            Message=message,
            Subject=subject
        )
        print(f"SNS notification sent. Message ID: {response['MessageId']}")
    except ClientError as e:
        print(f"Error sending SNS notification: {e}")
        raise

# Function to clear all files in /tmp directory
def clear_tmp_folder():
    tmp_dir = '/tmp'
    try:
        for filename in os.listdir(tmp_dir):
            file_path = os.path.join(tmp_dir, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)  # Delete file or symbolic link
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)  # Delete directory and its contents
                print(f"Deleted: {file_path}")
            except Exception as e:
                print(f"Failed to delete {file_path}. Reason: {e}")
    except Exception as e:
        print(f"Error cleaning /tmp folder: {e}")

# Main function to process drug report data
async def process_reports(drug_names_path, report_drug_path, output_s3_bucket, output_s3_key, missing_drug_sns_arn, api_maintenance_sns_arn):
    # Read drug names, ignoring empty lines
    with open(drug_names_path, 'r', encoding='utf-8') as names_file:
        drug_names = set(line.strip().lower() for line in names_file if line.strip())  # Filter out empty lines

    with open(report_drug_path, 'r', encoding='utf-8') as drug_file:
        report_lines = drug_file.readlines()

    report_ids = set()
    missing_drugs = []

    for drug in drug_names:
        drug_found = False
        for line in report_lines:
            if drug in line.lower():
                drug_found = True
                fields = line.strip().split("$")
                if len(fields) > 3:
                    report_id = fields[1].strip().replace("\"", "")
                    report_ids.add(report_id)
        if not drug_found:
            missing_drugs.append(drug)

    if missing_drugs:
        missing_drug_message = "The following drugs were not found in the report_drug.txt file:\n\n"
        missing_drug_message += "\n".join([f"{index + 1}. {drug}" for index, drug in enumerate(missing_drugs)])
        send_sns_notification(missing_drug_sns_arn, missing_drug_message, "Missing Drug Report")

    error_notification_sent = [False]
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_report_details(session, report_id, api_maintenance_sns_arn, error_notification_sent) for report_id in report_ids]
        report_details = await asyncio.gather(*tasks)

    local_output_path = '/tmp/report_details_output.json'
    with open(local_output_path, 'w', encoding='utf-8') as output_file:
        json.dump(report_details, output_file, ensure_ascii=False, indent=4)

    upload_file_to_s3(output_s3_bucket, output_s3_key, local_output_path)
    print(f"Report details uploaded to S3 at {output_s3_key}.")

# AWS Lambda handler function
def lambda_handler(event, context):
    # Clean the /tmp folder before starting the process
    clear_tmp_folder()

    # Extract S3 bucket and file paths from the event
    input_bucket_name = os.environ['Input_Bucket_name']
    drug_names_s3_key = os.environ['Drug_name']
    report_drug_s3_key = os.environ['Report_drug']
    output_bucket_name = os.environ['Output_Bucket_name']

    # SNS topic ARNs for notifications
    missing_drug_sns_arn = os.environ['Missing_Drug_Sns_Topic_Arn']
    api_maintenance_sns_arn = os.environ['Api_Maintenance_Sns_Topic_Arn']

    # Generate timestamp for the output file name in IST
    ist_offset = timedelta(hours=5, minutes=30)
    ist = datetime.now(timezone.utc) + ist_offset
    timestamp_ist = ist.strftime("%d_%m_%Y_%H_%M_%S")
    output_s3_prefix = os.environ['output_folder']
    output_s3_key = f"{output_s3_prefix}/report_details_output_{timestamp_ist}.json"

    # Define local file paths for temporary storage in Lambda
    drug_names_local_path = '/tmp/drug_names.txt'
    report_drug_local_path = '/tmp/report_drug.txt'

    # Download input files from the input bucket
    download_file_from_s3(input_bucket_name, drug_names_s3_key, drug_names_local_path)
    download_file_from_s3(input_bucket_name, report_drug_s3_key, report_drug_local_path)

    # Run the report processing asynchronously
    asyncio.run(process_reports(drug_names_local_path, report_drug_local_path, output_bucket_name, output_s3_key, missing_drug_sns_arn, api_maintenance_sns_arn))

    return {
        'statusCode': 200,
        'body': json.dumps(f"Report details uploaded to S3 at {output_s3_key} in the output bucket {output_bucket_name}.")
    }
